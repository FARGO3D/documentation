
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Communications &#8212; FARGO3D User Guide</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'communications';</script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Cuda translator (C2CUDA.py)" href="c2cuda.html" />
    <link rel="prev" title="Outputs" href="outputs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
    <meta name="docbuild:last-update" content="Dec 20, 2024"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">FARGO3D User Guide</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="download.html">Download</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to the code</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributors.html">Developers</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">Citing FARGO3D</a></li>

<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="first_steps.html">First Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="dir_tree.html">Directory tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="make_proc.html">Make Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="boundaries.html">Boundaries</a></li>
<li class="toctree-l1"><a class="reference internal" href="mesh_fields.html">Mesh and Fields</a></li>
<li class="toctree-l1"><a class="reference internal" href="def_setups.html">Default SETUPS</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt_file.html">.opt files</a></li>
<li class="toctree-l1"><a class="reference internal" href="non_uniform_meshes.html">Non-uniform meshes</a></li>
<li class="toctree-l1"><a class="reference internal" href="nbody.html">N-Body solver</a></li>
<li class="toctree-l1"><a class="reference internal" href="units.html">Units</a></li>
<li class="toctree-l1"><a class="reference internal" href="multifluid.html">Multifluid</a></li>
<li class="toctree-l1"><a class="reference internal" href="induction_equation.html">Induction equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="new_setup.html">Defining a new SETUP</a></li>

<li class="toctree-l1"><a class="reference internal" href="outputs.html">Outputs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Communications</a></li>
<li class="toctree-l1"><a class="reference internal" href="c2cuda.html">Cuda translator (C2CUDA.py)</a></li>
<li class="toctree-l1"><a class="reference internal" href="exec_flags.html">Execution flags</a></li>
<li class="toctree-l1"><a class="reference internal" href="vtk_comp.html">VTK Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda_perf.html">Improving CUDA Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">GPU vs CPU Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_setup.html">Developing a complex setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="ttt.html">Tips, Tricks, Todos and Troubleshooting</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/communications.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Communications</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-cpu-communications">GPU/CPU communications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi">MPI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi-cuda">MPI-CUDA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-considerations">General considerations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-the-device-selection-rule">Implementation of the device selection rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-aware-mpi-implementations">CUDA aware MPI implementations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spawning-a-job-on-a-cluster-of-gpus-a-primer">Spawning a job on a cluster of GPUs: a primer</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="communications">
<h1>Communications<a class="headerlink" href="#communications" title="Link to this heading">#</a></h1>
<p>Mono-GPU runs (with CUDA, multi-CPU runs (with MPI) and obviously
multi-GPU runs (with MPI+CUDA) all need some kind of communications.</p>
<p>When we run a SETUP on a GPU, the GPU sometimes needs to exchange
information with the host (CPU). It is therefore important to give
some information about this kind of transactions.</p>
<p>On the other hand, when we run a CPU-parallel version of the code,
each processor must communicate information about its contour cells
with neighboring processors, which involves MPI communications.</p>
<p>Finally, when we run a mixed version (ie: GPU-parallel, on a cluster
of GPUs), both processes are combined, and communications</p>
<blockquote>
<div><p>device&lt;—&gt;host (CPU)</p>
</div></blockquote>
<p>happen, as well as communications:</p>
<blockquote>
<div><p>CPU&lt;—through MPI—&gt;CPU</p>
</div></blockquote>
<p>Traditionally, GPU to GPU communication would imply a three-part trip:
the information is downloaded from the GPU to its corresponding CPU
(process), then sent to a neighbor, and finally uploaded to the GPU of
that neighbor.</p>
<p>Recent CUDA implementations permit however to issue MPI communication
statements using pointers on board the GPUs. The information then
travels using the fastest way, in a manner totally transparent to the
user. FARGO3D handles this case, which is activated with the make
option “mpicuda” (<code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">mpicuda</span></code> or <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">MPICUDA=1</span></code>). If you want
to get the best performance of FARGO3D on your GPU cluster, it is
mandatory to use the <code class="docutils literal notranslate"><span class="pre">mpicuda</span></code> option.</p>
<p>We present in some detail all these kinds of communications hereafter.</p>
<section id="gpu-cpu-communications">
<span id="gpucomm"></span><h2>GPU/CPU communications<a class="headerlink" href="#gpu-cpu-communications" title="Link to this heading">#</a></h2>
<p>First to all, assume we run the code in sequential mode on one GPU.
Naturally, routines that run on the GPU must have at their disposal
data in the Video RAM (device’s global memory in GPU’s jargon),
whereas routines that run on the CPU must have at their disposal data
on the normal RAM (host memory). When the data is updated on, say, the
GPU, it is not updated automatically on the CPU, nor vice-versa.</p>
<p>Managing manually data transfer between CPU and GPU (host and device)
is a programmer’s nightmare. It is extremely error-prone, and proves
to be impractical for a code of the complexity of FARGO3D. The GFARGO
code, which has a simpler structure, has been developed using manual
data transfers from GPU to CPU and vice-versa, and it took a very long
time to get the data transfers right.</p>
<p>In order to understand how data transfer is dealt with in a
semi-automatic way in FARGO3D, let us examine a real-life example. In
what follows, a green rectangle means that the data is correct and up
to date, whereas a red rectangle corresponds to random or out-of-date
data.</p>
<p>We start by initializing a data field (say <code class="docutils literal notranslate"><span class="pre">Vx</span></code>), on the CPU (there
is absolutely no reason to try and initialize them directly on the
GPU: this would add a lot of complexity and it would be pointless).</p>
<p>We have therefore the following situation:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/struct_init.png"><img alt="_images/struct_init.png" src="_images/struct_init.png" style="width: 270.0px; height: 129.0px;" />
</a>
</figure>
<p>Before starting a (M)HD time step on the GPU, we need to upload the
data to it. This is done by a “host to device” communication (<code class="docutils literal notranslate"><span class="pre">H2D</span></code>)
which occurs through the PCI bus:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/struct_H2D.png"><img alt="_images/struct_H2D.png" src="_images/struct_H2D.png" style="width: 268.0px; height: 128.0px;" />
</a>
</figure>
<p>The run can start on the GPU. Only the data on board the GPU is then
up to date, while the data on the CPU is left untouched and
corresponds to the initial condition:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/struct_runGPU.png"><img alt="_images/struct_runGPU.png" src="_images/struct_runGPU.png" style="width: 269.0px; height: 127.0px;" />
</a>
</figure>
<p>At some point, we need to get the data back on the CPU (for instance
to dump it to an output file, or because we have written a new routine
that uses it and which we have not developed to run on the GPU). We
need to perform a communication through the PCI bus from the device to
the host (<code class="docutils literal notranslate"><span class="pre">D2H</span></code>), this time:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/struct_D2H.png"><img alt="_images/struct_D2H.png" src="_images/struct_D2H.png" style="width: 268.0px; height: 126.0px;" />
</a>
</figure>
<p>A simple example could be:</p>
<p>Suppose we are in the first time step of a run, and we want to compute
the pressure field on the GPU (<span class="math notranslate nohighlight">\(P=c_s^2\rho\)</span>), but we have set
the initial conditions on the CPU. We need to upload the sound speed
and density fields. We do not upload all fields. This would be
extremely time-consuming. We only upload what is needed and nothing
else.</p>
<p>In order for that to be achieved semi-automatically, we have defined
two macrocommands that are called <code class="docutils literal notranslate"><span class="pre">INPUT</span></code> and <code class="docutils literal notranslate"><span class="pre">OUTPUT</span></code>. What these
macrocommands do is that they update the “color” (red or green) of a
field on the GPU or CPU, so that we know whether it is up to date or
not, and they transfer the data to the place where the calculation is
going to proceed, if it turns out to be out of date at the beginning
of a routine. For instance, consider the following piece of code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ComputePressure</span> <span class="p">()</span> <span class="p">{</span>
    <span class="n">INPUT</span> <span class="p">(</span><span class="n">SoundSpeed</span><span class="p">);</span>
    <span class="n">INPUT</span> <span class="p">(</span><span class="n">Density</span><span class="p">);</span>
    <span class="n">OUTPUT</span> <span class="p">(</span><span class="n">Pressure</span><span class="p">);</span>
    <span class="o">....</span>
    <span class="n">main</span> <span class="n">loop</span><span class="p">:</span>
           <span class="n">pressure</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">density</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">*</span><span class="n">soundspeed</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">*</span><span class="n">soundspeed</span><span class="p">[</span><span class="n">l</span><span class="p">];</span>
 <span class="p">}</span>
</pre></div>
</div>
<p>The macrocommands <code class="docutils literal notranslate"><span class="pre">INPUT</span></code> and <code class="docutils literal notranslate"><span class="pre">OUTPUT</span></code> expand differently on the
CPU and on the GPU (to be more accurate in C functions and in CUDA
kernels).  On the CPU, <code class="docutils literal notranslate"><span class="pre">INPUT</span></code> checks the “color” (red or green) of
its argument field on the CPU, and if it is red, it requires a
communication “device to host” of this specific field. This ensures
automatically that the field we process on the CPU is up to date when
we enter the routine’s main loop.  Similarly, on the CPU, <code class="docutils literal notranslate"><span class="pre">OUTPUT</span></code>
sets to “green” the state of its argument field on the CPU, and to
“red” its state on the GPU.</p>
<p>On the GPU, the macrocommands expand in the opposite way. We leave as
an exercise to the reader to check that one can exchange in the above
paragraph the words CPU and GPU, device and host.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Implementation-wise, we do not truly define a color for CPU
and GPU. Rather, each Field has two boolean flags, named
<code class="docutils literal notranslate"><span class="pre">fresh_cpu</span></code> and <code class="docutils literal notranslate"><span class="pre">fresh_gpu</span></code>. If <code class="docutils literal notranslate"><span class="pre">fresh_cpu</span></code> is YES, it means
that the data on the CPU is up to date (“green state” in the above
explanation), and out of date otherwise (“red state”). Similar
rules apply for <code class="docutils literal notranslate"><span class="pre">fresh_gpu</span></code>.  One should never set, nor even see
directly these flags. All of this is taken care of by <code class="docutils literal notranslate"><span class="pre">INPUT</span></code> and
<code class="docutils literal notranslate"><span class="pre">OUTPUT</span></code>.</p>
</div>
<dl class="simple">
<dt><strong>Take away message</strong>: you should only care to properly state, at the</dt><dd><p>beginning of each routine, which fields are <code class="docutils literal notranslate"><span class="pre">INPUT</span></code> and which
fields are <code class="docutils literal notranslate"><span class="pre">OUTPUT</span></code>. That’s all.  This should be done rigorously as
you start to write the routine. If you forget to do it, the code will
throw wrong results when run on a GPU built.  If you do it correctly,
you will never have to worry about CPU/GPU communications, which will
take place automatically for you behind the scene. This is easy to
do, intuitive, but it must be done rigorously.</p>
</dd>
</dl>
<p>All the details can be found in <code class="docutils literal notranslate"><span class="pre">src/fresh.c</span></code> file. We have actually
developed several kinds of <code class="docutils literal notranslate"><span class="pre">INPUT/OUTPUT</span></code> macrocommands, for each
type of field encountered in the code: Field (volumic data), Field2D
(X-averaged, ie azimuthally averaged data 2D real data), and
FieldInt2D (2D fields of integers). The latter is used in particular
for the shifts needed by the azimuthal advection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INPUT</span><span class="p">(</span><span class="n">Field</span><span class="p">)</span>
<span class="n">INPUT2D</span><span class="p">(</span><span class="n">Field2D</span><span class="p">)</span>
<span class="n">INPUT2DINT</span><span class="p">(</span><span class="n">FieldInt2D</span><span class="p">)</span>
<span class="n">OUTPUT</span><span class="p">(</span><span class="n">Field</span><span class="p">)</span>
<span class="n">OUTPUT2D</span><span class="p">(</span><span class="n">Field2D</span><span class="p">)</span>
<span class="n">OUTPUT2DINT</span><span class="p">(</span><span class="n">FieldInt2D</span><span class="p">)</span>
</pre></div>
</div>
<p>Under the hood, these methods are only wrappers of the cudaMemcpy()
function.</p>
</section>
<section id="mpi">
<h2>MPI<a class="headerlink" href="#mpi" title="Link to this heading">#</a></h2>
<p>When FARGO3D is running in parallel mode, the main computational mesh
must be split into several submeshes, each one corresponding to a
cluster core. All the computation is done independently inside this
submesh (because all the HD/MHD equations are local), but at the
borders of the submeshes some communications must be done with
neighbors in order to merge all problems into a big one.</p>
<p>The mesh is split so as to minimize the surface of contact between the
processors. Following this rule, the size of the communications is
minimal. Note that much like the former FARGO code, the mesh is not
split in the X (azimuthal) direction, because orbital advection is not
local in x. This represents a penalty for communications, because the
“contact surface” between processors is not as small as it could be if
we split the mesh in x as well.</p>
<p>The abscissa and ordinate of each processor in the 2D mesh (Y and Z)
of processors are the global variables <code class="docutils literal notranslate"><span class="pre">I</span></code> and <code class="docutils literal notranslate"><span class="pre">J</span></code>. In practice,
with this indices, and with the variables CPU_Rank and CPU_Number, you
have all the information needed to know where each process lies in the
mesh of processes and who the neighbors are.</p>
</section>
<section id="mpi-cuda">
<h2>MPI-CUDA<a class="headerlink" href="#mpi-cuda" title="Link to this heading">#</a></h2>
<section id="general-considerations">
<h3>General considerations<a class="headerlink" href="#general-considerations" title="Link to this heading">#</a></h3>
<p>In a mixed CUDA+MPI run, we must have one processing element
(“processor”) per GPU. Normally, when you run CUDA on one GPU only,
the driver selects the device for you automatically, or you may
specify manually which device you want to run on by specifying the
<code class="docutils literal notranslate"><span class="pre">-D</span></code> flag on the command line.  This is obviously not possible here,
as all processes within the same node would run on the same
device. Instead, each process will have to select at run-time, in an
automatic manner, the correct GPU through a directive of the kind:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cudaSetDevice</span> <span class="p">(</span><span class="nb">int</span> <span class="n">device_number</span><span class="p">);</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">device_number</span></code> must be evaluated depending on the process
rank.  Assume that your cluster has a topology similar to this one:</p>
<figure class="align-center" style="width: 280px">
<a class="reference internal image-reference" href="_images/cluster-top.png"><img alt="_images/cluster-top.png" src="_images/cluster-top.png" style="width: 250px;" />
</a>
</figure>
<p>Despite the fact that four processes could fit on each node for
non-GPU runs, here we must limit ourselves to two processes only per
node, otherwise, several processes will use the same GPU, leading to a
degradation of performance. Depending on your MPI implementation, the
rank ordering of processes could then be as follows:</p>
<figure class="align-center" style="width: 580px">
<a class="reference internal image-reference" href="_images/pt1.png"><img alt="_images/pt1.png" src="_images/pt1.png" style="width: 250px;" />
</a>
</figure>
<p>or the processes could be distributed in a different manner, as shown
below:</p>
<figure class="align-center" style="width: 580px">
<a class="reference internal image-reference" href="_images/pt2.png"><img alt="_images/pt2.png" src="_images/pt2.png" style="width: 250px;" />
</a>
</figure>
<p>The strategy to calculate the device number would be different in
these two cases.  In the first case, we should have a rule like this
one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device_number</span> <span class="o">=</span> <span class="n">CPU_Rank</span> <span class="o">%</span> <span class="n">number_of_processes_per_node</span><span class="p">;</span>
</pre></div>
</div>
<p>where the <code class="docutils literal notranslate"><span class="pre">%</span></code> operator represents the <em>modulo</em> operation in C. On
the contrary, in the second case, we would need a rule like this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device_number</span> <span class="o">=</span> <span class="n">CPU_Rank</span> <span class="o">/</span> <span class="n">number_of_nodes</span><span class="p">;</span>
</pre></div>
</div>
<p>where the division is an integer (Euclidean) division.</p>
<p>Naturally, in the first case, the <code class="docutils literal notranslate"><span class="pre">number_of_processes_per_node</span></code> is
also the number of GPUs per node. We can check that it yields the
desired correspondence:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Process</p></th>
<th class="head"><p>GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<p>whereas in the second case the correspondence is as expected:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Process</p></th>
<th class="head"><p>GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<p>Prior to writing the rule to select your GPUs on your cluster, you
should determine how your MPI implementation distributes the process
ranks among the nodes (case 1 or 2) by writing a test program such
as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#include &lt;stdio.h&gt;</span>
<span class="c1">#include &lt;mpi.h&gt;</span>

<span class="nb">int</span> <span class="n">main</span> <span class="p">(</span><span class="nb">int</span> <span class="n">argc</span><span class="p">,</span> <span class="n">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="nb">int</span> <span class="n">rank</span><span class="p">;</span>
  <span class="n">char</span> <span class="n">hostname</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span>
  <span class="n">MPI_Init</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
  <span class="n">MPI_Comm_rank</span> <span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>
  <span class="n">gethostname</span> <span class="p">(</span><span class="n">hostname</span><span class="p">);</span>
  <span class="n">printf</span> <span class="p">(</span><span class="s2">&quot;I, process of rank </span><span class="si">%d</span><span class="s2">, run on host </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">hostname</span><span class="p">)</span>
  <span class="n">MPI_Finalize</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="implementation-of-the-device-selection-rule">
<h3>Implementation of the device selection rule<a class="headerlink" href="#implementation-of-the-device-selection-rule" title="Link to this heading">#</a></h3>
<p>How do we implement the device selection rule seen above? This should
be done on a platform+MPI version basis (on the same platform, two
different flavors of MPI may behave differently). This is done in the
function <code class="docutils literal notranslate"><span class="pre">SelectDevice</span> <span class="pre">(int</span> <span class="pre">myrank)</span></code> in the file
<code class="docutils literal notranslate"><span class="pre">src/select_device.c</span></code>.  You can see that in this function we have a
series of tests on the hostname for which we have implemented some
selection rules. For instance, we have developed FARGO3D among others
on a workstation with two Tesla C2050 cards (hostname <code class="docutils literal notranslate"><span class="pre">tesla</span></code>), and
for this device, we have the selection rule:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">-</span> <span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">myrank</span> <span class="o">%</span> <span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
<p>which selects device 1 for rank 0 and device 0 for rank 1 (the reason
for swapping the GPUs with respect to normal order is that a run with
1 process only will run on the GPU 1, for which the temperature levels
off at a smaller value than GPU 0 during a long run…).</p>
<p>As you see, you have all the freedom to implement your own rules
within this routine, with tests similar to those already written. It
would be probably better to have tests using an environment variable
or to use <code class="docutils literal notranslate"><span class="pre">#ifdef</span></code> directives which would use some variable defined
in the platform specific section of the makefile. We might implement
such features in the future.</p>
<p>The device eventually adopted by the process is as follows:</p>
<ul class="simple">
<li><p>If an explicit rule is defined for your platform, the device defined
in this rule is adopted.</p></li>
<li><p>If you specify explicitly the device with the <code class="docutils literal notranslate"><span class="pre">-D</span></code> option on the
command line, the device thus chosen has priority in any case (in
particular it overwrites the device given by your platform rule, if
any).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your run is MPI and you use option <code class="docutils literal notranslate"><span class="pre">-D</span></code>, a warning is
issued since all your processes run on the same GPU.</p>
</div>
<ul class="simple">
<li><p>If no rule is found for your platform and you have not specified any
device on the command line, CUDA chooses the device for you (the
rules for this selection are those of the function
<code class="docutils literal notranslate"><span class="pre">cudaChooseDevice()</span></code>.)  DO NOT RELY ON THIS AUTOMATIC SOLUTION to
decide for you in a MPI run. The different processes will see that
device 0 is available when they enter simultaneously the function
<code class="docutils literal notranslate"><span class="pre">select_device()</span></code> and they will all select this device.</p></li>
</ul>
<p>Finally, a message is issued in any case stating the process rank and
the device on which it runs.</p>
</section>
<section id="cuda-aware-mpi-implementations">
<span id="mpicuda"></span><h3>CUDA aware MPI implementations<a class="headerlink" href="#cuda-aware-mpi-implementations" title="Link to this heading">#</a></h3>
<p>As advertised earlier, recent implementations of CUDA can deal with
direct device-to-device (“GPU to GPU”) MPI communications (so-called
GPU Direct). We shall not consider the details here but the interested
reader could consult the following <a class="reference external" href="http://devblogs.nvidia.com/parallelforall/introduction-cuda-aware-mpi/">page</a>.</p>
<p>Compiling the code with a CUDA aware version of MPI is relatively
straightforward, but there is one subtlety with which we must
deal. The problem is the following.</p>
<p>In order to setup the CUDA-aware MPI machinery behind the scene, each
process must already have “chosen” its GPU when the code executes
<code class="docutils literal notranslate"><span class="pre">MPI_Init()</span></code>. However, as we have seen at length above, choosing the
GPU is done on the basis of the rank. But how can a process know its
rank, even before entering <code class="docutils literal notranslate"><span class="pre">MPI_Init()</span></code> ?  In order to avoid this
vicious circle, implementations of MPI provide a mechanism that allows
us to know the rank of the process even before <code class="docutils literal notranslate"><span class="pre">MPI_Init()</span></code> has been
invoked. This mechanism cannot be a <code class="docutils literal notranslate"><span class="pre">MPI_Something</span> <span class="pre">()</span></code> directive, as
no MPI directive can be called before <code class="docutils literal notranslate"><span class="pre">MPI_Init()</span></code>. Rather, it
simply consists in reading an environment variable that has a specific
name. There are two such flavors of variables in each MPI
implementation/ For instance, these variables are named
OMPI_COMM_WORLD_RANK and OMPI_COMM_WORLD_LOCAL_RANK in OpenMPI. Each
process can, therefore, get its rank in this manner:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rank</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OMPI_COMM_WORLD_RANK&quot;</span><span class="p">));</span>
</pre></div>
</div>
<p>or its local rank (i.e. within a given node) as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">local_rank</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OMPI_COMM_WORLD_LOCAL_RANK&quot;</span><span class="p">));</span>
</pre></div>
</div>
<p>The value returned will be different for each process, which will
allow a selection of the device on this basis, so that <code class="docutils literal notranslate"><span class="pre">MPI_Init()</span></code>
can be called afterwards. If FARGO3D is compiled with the make flag
MPICUDA, main() will invoke a function called
<code class="docutils literal notranslate"><span class="pre">EarlyDeviceSelection()</span></code> just after reading the parameter file, and
it will subsequently invoke <code class="docutils literal notranslate"><span class="pre">SelectDevice()</span></code> with the rank thus
obtained.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">OMPI_COMM_WORLD_LOCAL_RANK</span></code> instead of
<code class="docutils literal notranslate"><span class="pre">OMPI_COMM_WORLD_RANK</span></code> is simpler. The former returns the
rank within a node (hence its name), so that it can directly
select the device number <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> without further
arithmetic. This is the approach used in FARGO3D when the
build flag <code class="docutils literal notranslate"><span class="pre">MPICUDA</span></code> is set.</p>
</div>
<p>To sum up, if you want to build FARGO3D with a CUDA aware MPI
implementation, you must pass this special environment variable to the
code at build time. This is achieved by defining the variable ENVRANK
in the makefile. You should edit one of the platform specific build
options provided in the makefile and adapt it to your own needs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>how do you know if the code is really running with GPU
Direct ? At run-time on a GPU built, if any communication of
a data cube occurs between the CPU and GPU, a flag is
raised, and a “!” is printed on the terminal instead of
FARGO(3D)’s classic dot. This helps to diagnose that
something is wrong, for instance when a part of a time step
is still running on the CPU (expensive, “volumic” kind of
data transfer are therefore occurring at each time
step). Similarly, if any communication of a data “square”
(ie the boundary of a cube) occurs between the CPU and GPU,
another flag is raised, and a “:” is printed on the
terminal. This happens if MPI communications are done
through the host, instead of being achieved through GPU
Direct: “surfacic” kind of data transfer is necessary
between the host and the device in this case.</p>
</div>
<p>To sum up, if you see on the terminal a line such as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!!!!!!!!!!!!!!!!!!
</pre></div>
</div>
<p>some part of the time step is still running on the CPU, with a
dramatic impact on performance. If you see:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">:::::::::::::::::</span>
</pre></div>
</div>
<p>then all routines are running on the GPU but MPI communications are
still done through the host, with a sizable performance
penalty. Finally, when you see the customary line of dots, everything
is running on the GPU and MPI communications are achieved through GPU
direct.</p>
</section>
</section>
<section id="spawning-a-job-on-a-cluster-of-gpus-a-primer">
<h2>Spawning a job on a cluster of GPUs: a primer<a class="headerlink" href="#spawning-a-job-on-a-cluster-of-gpus-a-primer" title="Link to this heading">#</a></h2>
<ul>
<li><p>If <code class="docutils literal notranslate"><span class="pre">MPICUDA</span></code> is not defined:</p>
<ul class="simple">
<li><dl class="simple">
<dt>You can use the flag <code class="docutils literal notranslate"><span class="pre">-D</span></code> to specify the device number on which</dt><dd><p>each GPU job must be launched. This is fine if your cluster has
only one GPU per node and you spawn one PE per node. A warning
message is issued in any case, as specifying manually the device
number should be reserved for sequential runs.</p>
</dd>
</dl>
</li>
<li><p>Alternatively, you can define a rule (e.g. hostname based) for the
device number, on the model of those already written in
<code class="docutils literal notranslate"><span class="pre">src/select_device.c</span></code> in the function <code class="docutils literal notranslate"><span class="pre">SelectDevice()</span></code>, which
is the function called when the code is compiled without
<code class="docutils literal notranslate"><span class="pre">MPICUDA</span></code>.</p></li>
</ul>
</li>
<li><p>if <code class="docutils literal notranslate"><span class="pre">MPICUDA</span></code> is defined:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>You can use the flag <code class="docutils literal notranslate"><span class="pre">-D</span></code> to specify the device number on which</dt><dd><p>each GPU job must be launched. This is fine if your cluster has
only one GPU per node and you spawn one PE per node. A warning
message is issued in any case, as specifying manually the device
number should be reserved for sequential runs.</p>
</dd>
</dl>
</li>
<li><p>You can use the flag <code class="docutils literal notranslate"><span class="pre">+D</span></code> to specify a list of devices on each
host. This is meant to be used, in general, with a job scheduler
such as PBS (see <a class="reference internal" href="exec_flags.html#execflags"><span class="std std-ref">Execution flags</span></a>.)</p></li>
<li><p>Finally, when neither <code class="docutils literal notranslate"><span class="pre">+D</span></code> nor <code class="docutils literal notranslate"><span class="pre">-D</span></code> is used, the device are
selected on the base of the local rank. All GPUs on the nodes
used by the run should be free when the run is launched,
otherwise they may get oversubscribed.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">+D</span></code> flag does not work for a build without
<code class="docutils literal notranslate"><span class="pre">MPICUDA</span></code> (non CUDA aware build).</p>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="outputs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Outputs</p>
      </div>
    </a>
    <a class="right-next"
       href="c2cuda.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Cuda translator (C2CUDA.py)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-cpu-communications">GPU/CPU communications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi">MPI</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mpi-cuda">MPI-CUDA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-considerations">General considerations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-the-device-selection-rule">Implementation of the device selection rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-aware-mpi-implementations">CUDA aware MPI implementations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spawning-a-job-on-a-cluster-of-gpus-a-primer">Spawning a job on a cluster of GPUs: a primer</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Author name not set
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2014-2023, Pablo Benítez Llambay, Frédéric Masset &amp; Contributors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Dec 20, 2024.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>